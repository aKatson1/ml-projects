{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d475f3-432e-4ff3-96e5-39f5f8ab5277",
   "metadata": {},
   "source": [
    "In this file, I explore Neural Nets, DNNs, CNNs, DNN/CNN combinations, dropout regularization, image augmentation, and transfer learning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf63d4db-d824-4b73-88fb-6c23cbd18bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1d59d-e584-41a2-9f26-0cfb61509d36",
   "metadata": {},
   "source": [
    "# Shallow Neural Net\n",
    "\n",
    "Firstly, we import the necessary `numpy` and `TensorFlow` libraries.  \n",
    "\n",
    "The following code performs these operations:  \n",
    "- Constructs a layer `l0` with one neuron, taking a 1-dimensional array as input, with the activation function defaulted to linear (identity map).  \n",
    "- Sets up a sequential (feedforward) neural network with just the layer `l0`.  \n",
    "- Calls `model.compile()` to define the optimizer, loss function, and metrics for training. (SGD = Stochastic Gradient Descent, used to escape local minima.)  \n",
    "- Defines the training data.  \n",
    "- Fits the model to the training data, specifying the number of epochs (iterations of the optimization algorithm).  \n",
    "- The model predicts a value of 10.0, which should be 19.  \n",
    "- Prints the learned model weights. Notice they are not exactly `2x - 1`, because the network approaches them via gradient descent rather than \"knowing\" the solution directly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c009224f-bdcc-469a-9947-d05831817d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 566ms/step - loss: 33.5519\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 26.7252\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 21.3476\n",
      "...\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.5273e-05\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 5.4137e-05\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "[[18.978535]]\n",
      "Here is what I learned: [array([[1.9968889]], dtype=float32), array([-0.99035394], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "l0 = Dense(units = 1, input_shape=[1]) \n",
    "model = Sequential([l0]) \n",
    "model.compile(optimizer = \"sgd\", loss = \"mean_squared_error\") \n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype = float) \n",
    "model.fit(xs, ys, epochs = 500) \n",
    "print(model.predict([10.0]))\n",
    "print(\"Here is what I learned: {}\".format(l0.get_weights())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42f4f9-8a72-448e-a182-15feceb7cdab",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "This is an example of a Deep Neural Network—several layers of neural nets.  \n",
    "\n",
    "This code does the following: \n",
    "- Start a timer.  \n",
    "- Load the MNIST dataset, which contains handwritten digits 0-9 along with their labels. Split it into training and test sets.  \n",
    "- Rescale the images from grayscale values 0–255 to 0–1 and flatten each 28 x 28 image into a 784-entry vector. This helps manage exploding gradients and speeds up processing.  \n",
    "- Set up a 3-layer model:  \n",
    "- First hidden layer: 128 neurons with ReLU activation  \n",
    "- Second hidden layer: 128 neurons with ReLU activation  \n",
    "- Output layer: 10 neurons with softmax activation (multi-class output)  \n",
    "- This outputs a 10-entry vector of probabilities for each class 0-9, which sum to 1.  \n",
    "- Compile the model by specifying the optimizer, loss function, and accuracy metric. The model is not yet trained.  \n",
    "- Train the model and evaluate it on the test set.  \n",
    "- Report the number of model parameters. Calculation: `(784+1)*128 + 128*(128+1) + (128+1)*10`, since every input gets a weight plus 1 bias.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31e3dd7c-f5a5-453e-9cd4-fc33aa68bc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 13s 6ms/step - loss: 0.4893 - accuracy: 0.8256\n",
      "...\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.2335 - accuracy: 0.9116\n",
      "313/313 [==============================] - 2s 4ms/step - loss: 0.3352 - accuracy: 0.8845\n",
      "Training Time: 109.65528678894043 seconds\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_17 (Dense)            (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images = training_images.reshape(60000, 784) / 255.0\n",
    "test_images = test_images.reshape(10000, 784) / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=10)\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Training Time: {training_time} seconds\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b7417a-0dab-4b65-82be-fb328a2ee59e",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "\n",
    "This is an example of a convolutional neural network (CNN).  \n",
    "\n",
    "A **convolution** is an n×n matrix with weights as entries (plus a bias term). For example, a 3×3 matrix has 10 parameters. The matrix slides over the input pixels, multiplying element-wise and summing the results. An activation function is then applied to this linear combination. The weights of the filter are learned during training and capture local features (edges, vertical/horizontal lines, color patterns, etc.). CNNs are essentially the image-based analogue of DNNs.  \n",
    "\n",
    "This code does the following: \n",
    "- Data preparation.  \n",
    "- Defining the feedforward model.  \n",
    "- Apply 64 different 3×3 filters to each of the 26×26 valid pixels (28×28 input, edges excluded). Each filter has 3×3 + 1 bias parameters → 10 parameters per filter. Thus with 64 filters, we have 640 trainable parameters here. The output is a 26×26×64 array (height × width × filter index/(x, y, filter_n value(x, y))). Apply ReLU activation after convolution, replacing negative values with zero to add non-linearity: `ReLU(W_3x3 * pixels + bias)` for each of 64 filters.  \n",
    "- Max pooling (no trainable parameters): reduce the 26×26×64 output to 13×13×64 by taking the max value in each 2×2 region for each filter.  \n",
    "- Apply another set of 64 new 3×3 filters on the 13×13×64 output of the previous 64 filters and pooling. Parameters: `(3×3 + 1 × 64) × 64 = 36,928`. Output: 11×11×64.  \n",
    "- Max pooling again: 11×11×64 → 5×5×64.  \n",
    "- Flatten the 5×5×64 array into a 1D vector of 1600 values, ready to feed into a fully connected neural network.  \n",
    "- Connect to a softmax output layer with 10 neurons (1601 parameters each).  \n",
    "- Compile, train, and report model information.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a60213b3-9a9a-49ab-8604-5ddf92a28133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.4583 - accuracy: 0.8348\n",
      "...\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.2274 - accuracy: 0.9176\n",
      "313/313 [==============================] - 5s 14ms/step - loss: 0.2712 - accuracy: 0.9012\n",
      "Training Time: 214.01053524017334 seconds\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 13, 13, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 11, 11, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                16010     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,578\n",
      "Trainable params: 53,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images = training_images.reshape(60000, 28, 28, 1) / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1) / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\", input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),  \n",
    "\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")  \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print(f\"Training Time: {training_time} seconds\")\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5b7fa-4fea-41c0-aba7-d7346ada885d",
   "metadata": {},
   "source": [
    "# Combined CNN + DNN\n",
    "\n",
    "The previous examples were only DNNs or CNNs. This example combines both approaches.  \n",
    "\n",
    "- Use the convolutional layers from the CNN to extract spatial features from the input images.  \n",
    "- Flatten the output of the convolutional layers to feed it into a fully connected DNN.  \n",
    "- Compare **training time**, **accuracy**, and **number of parameters** across the three models:  \n",
    "  1. DNN only  \n",
    "  2. CNN only  \n",
    "  3. CNN + DNN combined  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f6c786-dd02-4df3-9762-8b8cd2946ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 58s 30ms/step - loss: 0.4379 - accuracy: 0.8413\n",
      "...\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.1057 - accuracy: 0.9613\n",
      "313/313 [==============================] - 9s 28ms/step - loss: 0.2980 - accuracy: 0.9149\n",
      "313/313 [==============================] - 8s 23ms/step\n",
      "[1.6829333e-11 2.9011214e-12 1.4376247e-13 9.9139134e-12 3.7999070e-14\n",
      " 1.7961634e-08 1.6138226e-14 7.5122242e-10 4.2089064e-14 9.9999994e-01]\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = data.load_data()\n",
    "\n",
    "training_images = training_images.reshape(60000, 28, 28, 1)\n",
    "training_images = training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Conv2D(64, (3, 3), activation = \"relu\", input_shape=(28,28,1)), \n",
    "tf.keras.layers.MaxPooling2D(2,2), \n",
    "tf.keras.layers.Conv2D(64, (3, 3), activation = \"relu\"), \n",
    "tf.keras.layers.MaxPooling2D(2,2), \n",
    "tf.keras.layers.Flatten(), \n",
    "tf.keras.layers.Dense(128, activation = tf.nn.relu), \n",
    "tf.keras.layers.Dense(10, activation = tf.nn.softmax) \n",
    "])\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs= 10)\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4497c719-bae4-497c-ad82-e6237d49c6c7",
   "metadata": {},
   "source": [
    "# RGB Image Classification: People vs. Horses\n",
    "\n",
    "Now we take a look at another dataset, this time with **3 color channels (R, G, B)** and larger images.  \n",
    "Due to the number of parameters and data size, this may take a few minutes to run.  \n",
    "This dataset attempts to classify images into **people vs. horses**, but the images are taken at various angles and contain different sizes and colors.  \n",
    "The model must **fit just enough to capture important distinguishing features**, without overfitting to irrelevant variations in angle, size, or color.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da73b65-d3bd-4cca-b3e7-a9e09e46015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 31s 861ms/step - loss: 0.6231 - accuracy: 0.7020 - val_loss: 0.6830 - val_accuracy: 0.8672\n",
      "...\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 29s 857ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 1.8823 - val_accuracy: 0.8633\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf \n",
    "import urllib.request, zipfile, os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from PIL import Image\n",
    "\n",
    "train_url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
    "validation_url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "file_name = \"data/horse-or-human.zip\" \n",
    "validation_file_name = \"data/validation-horse-or-human.zip\"  \n",
    "\n",
    "training_dir = \"data/horse-or-human/training\"  \n",
    "validation_dir = \"data/horse-or-human/validation\"  \n",
    "\n",
    "urllib.request.urlretrieve(train_url, file_name)\n",
    "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(training_dir)\n",
    "\n",
    "with zipfile.ZipFile(validation_file_name, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(validation_dir)\n",
    "    \n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir, \n",
    "    target_size=(300, 300),\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(300, 300),\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "     tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300,300,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "     tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "     tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "       tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "       tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss= \"binary_crossentropy\", optimizer = RMSprop(learning_rate=0.001), metrics = [\"accuracy\"])\n",
    "history = model.fit(train_generator, epochs = 10, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe49771-dc47-4d90-a192-7f5b58601e96",
   "metadata": {},
   "source": [
    "# Dropout Regularization and Image Augmentation\n",
    "\n",
    "The following are examples of **dropout regularization** and **image augmentation**.  \n",
    "\n",
    "**Dropout** reduces overfitting during training by randomly setting a subset of neuron outputs to 0 and scaling the remaining outputs by `1 / (1 - dropout rate)`.  \n",
    "\n",
    "- This prevents the network from relying too heavily on any single sequence of neurons, encouraging it to learn more generalizable features.  \n",
    "- The subset is selected using a boolean mask of length equal to the number of neurons, and is regenerated for each batch and epoch.  \n",
    "- Dropout is only applied during training to adjust parameters, not during evaluation.  \n",
    "\n",
    "**Image augmentation** is another technique to reduce overfitting.  \n",
    "\n",
    "- It modifies images in the training set to expose the network to a broader variety of patterns, improving performance on unseen data.  \n",
    "- Augmentation helps up to a point— overdoing it can introduce unrealistic features (e.g., a swirl effect on a horse) that the model will never encounter in real-world data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66a4d190-8db5-4f8b-b239-25fdea1e7d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1027 images belonging to 2 classes.\n",
      "Found 256 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "33/33 [==============================] - 60s 2s/step - loss: 0.7108 - accuracy: 0.5005 - val_loss: 0.6794 - val_accuracy: 0.9570\n",
      "...\n",
      "Epoch 10/10\n",
      "33/33 [==============================] - 56s 2s/step - loss: 0.2633 - accuracy: 0.8958 - val_loss: 0.9596 - val_accuracy: 0.7227\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_10 (Conv2D)          (None, 298, 298, 16)      448       \n",
      "                                                                 \n",
      " max_pooling2d_10 (MaxPoolin  (None, 149, 149, 16)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 147, 147, 32)      4640      \n",
      "                                                                 \n",
      " max_pooling2d_11 (MaxPoolin  (None, 73, 73, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 71, 71, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 35, 35, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 33, 33, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 14, 14, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 7, 7, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,704,097\n",
      "Trainable params: 1,704,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from PIL import Image\n",
    "import urllib.request, zipfile, os\n",
    "\n",
    "train_url = \"https://storage.googleapis.com/learning-datasets/horse-or-human.zip\"\n",
    "validation_url = \"https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip\"\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "file_name = \"data/horse-or-human.zip\" \n",
    "validation_file_name = \"data/validation-horse-or-human.zip\"  \n",
    "\n",
    "training_dir = \"data/horse-or-human/training\"  \n",
    "validation_dir = \"data/horse-or-human/validation\"  \n",
    "\n",
    "urllib.request.urlretrieve(train_url, file_name)\n",
    "urllib.request.urlretrieve(validation_url, validation_file_name)\n",
    "\n",
    "with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(training_dir)\n",
    "\n",
    "with zipfile.ZipFile(validation_file_name, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(validation_dir)\n",
    "\n",
    "train_datagen2 = ImageDataGenerator(rescale=1/255, rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True, fill_mode = \"nearest\")\n",
    "train_generator2 = train_datagen2.flow_from_directory(\n",
    "    training_dir, \n",
    "    target_size=(300, 300),\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir, \n",
    "    target_size=(300, 300),\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "model2 = tf.keras.models.Sequential([\n",
    "     tf.keras.layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300,300,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "     tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "     tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "       tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "       tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.Flatten(), \n",
    "    tf.keras.layers.Dense(512, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model2.compile(loss= \"binary_crossentropy\", optimizer = RMSprop(learning_rate=0.001), metrics = [\"accuracy\"])\n",
    "history2 = model2.fit(train_generator2, epochs = 10, validation_data = validation_generator)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd787e-11f4-4b67-ae4d-6eaf36382f37",
   "metadata": {},
   "source": [
    "# Transfer Learning Example\n",
    "\n",
    "**Transfer learning** allows you to leverage larger, pre-trained networks to save computing power.  \n",
    "\n",
    "- It involves \"freezing\" the variables learned by a pre-trained model (often trained on massive datasets with powerful hardware) and only training a few layers at the end.  \n",
    "\n",
    "Starting at the line `base_model = MobileNetV2()`, the following occurs:  \n",
    "\n",
    "- Loads **MobileNetV2**, a convolutional neural network (CNN) with pre-trained weights from **ImageNet** (1.2M images, 1000 classes). The top layer is excluded since we do not need a 1000-class softmax.  \n",
    "- Freezes the base model (`trainable=False`) so the pre-trained weights are not updated during training.  \n",
    "- The output of the base model is a `(7, 7, 1280)` feature map (series of filters and pooling).  \n",
    "- Applies **global average pooling 2D**, reducing `(7, 7, 1280)` → `(1280,)`. This reduces the data more efficiently than flattening.  \n",
    "- Finally, we add a single neuron for **binary classification** and test the model on a few images. We set a threshold at 0.5 for the sigmoid and output the sigmoid probabilities.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9069902-550e-4ec4-a525-1fdb1aea048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1170 images belonging to 2 classes.\n",
      "Found 128 images belonging to 2 classes.\n",
      "Found 4 images belonging to 1 classes.\n",
      "Epoch 1/10\n",
      "37/37 [==============================] - 29s 662ms/step - loss: 0.6431 - accuracy: 0.6453 - val_loss: 0.5787 - val_accuracy: 0.7656\n",
      "...\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 25s 665ms/step - loss: 0.2288 - accuracy: 0.9162 - val_loss: 0.4906 - val_accuracy: 0.8047\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 1.6949 - accuracy: 0.5000\n",
      "Test Loss: 1.6949\n",
      "Test Accuracy: 0.5000\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "Test\\pic_1.jpg: Predicted class -> 0 (Confidence: 0.27)\n",
      "Test\\pic_2.jpg: Predicted class -> 0 (Confidence: 0.06)\n",
      "Test\\pic_3.jpg: Predicted class -> 1 (Confidence: 0.99)\n",
      "Test\\pic_4.jpg: Predicted class -> 1 (Confidence: 0.73)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "file_name = \"data/Class_Carnations.zip\"  \n",
    "file_name2 = \"data/Class_Roses.zip\"   \n",
    "training_dir = \"data/Rose-or-Carnation/training\"  \n",
    "test_zip = \"data/Test.zip\"\n",
    "testing_dir = \"data/Rose-or-Carnation/testing\"\n",
    "\n",
    "with zipfile.ZipFile(file_name, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(training_dir)\n",
    "\n",
    "with zipfile.ZipFile(file_name2, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(training_dir)\n",
    "\n",
    "os.makedirs(testing_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(test_zip, \"r\") as zip_ref:\n",
    "    zip_ref.extractall(testing_dir)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255, validation_split=0.1)\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(224, 224),\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"training\",\n",
    "    seed=17 #set random seed \n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=(224, 224),\n",
    "    class_mode=\"binary\",\n",
    "    subset=\"validation\",\n",
    "    seed=17 #same seed-- keeping validation set seperate from training \n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=(224, 224),  # Important: match MobileNetV2 input size\n",
    "    class_mode=\"binary\",\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  \n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) \n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "predictions = model.predict(test_generator) \n",
    "\n",
    "predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "\n",
    "for filename, pred, prob in zip(filenames, predicted_classes, predictions.flatten()):\n",
    "    print(f\"{filename}: Predicted class -> {pred} (Confidence: {prob:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_env]",
   "language": "python",
   "name": "conda-env-tensorflow_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
